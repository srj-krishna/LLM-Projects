{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srj-krishna/LLM-Projects/blob/main/StableLM_3B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpM-51EHwpIm",
        "outputId": "2059091c-d114-4aa7-a8b7-cf44b743b213"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "ESwdnkz004NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Get your Hugging Face token from your Hugging Face account settings\n",
        "hugging_face_token = \"Your-HuggingFace-Token-Here\"  # Replace this with your actual token\n",
        "\n",
        "# Set the device to GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", trust_remote_code=True, token=hugging_face_token).to(device)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\", token=hugging_face_token)\n",
        "\n",
        "# Generate some text\n",
        "inputs = tokenizer(\"It was a dark and stormy night\", return_tensors=\"pt\").to(device)\n",
        "\n",
        "tokens = model.generate(\n",
        "   **inputs,\n",
        "   max_new_tokens=400,\n",
        "   temperature=0.75,\n",
        "   top_p=0.95,\n",
        "   do_sample=True,\n",
        ")\n",
        "print(tokenizer.decode(tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ5GLYlO5iDz",
        "outputId": "cc6cbf95-6b28-4771-e049-e792dca623d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a dark and stormy night.\n",
            "Well, not exactly. But it was the night of the “Chicken” and “Cranberry” parties, and this was the scene we found ourselves in:\n",
            "A lovely room, with a few dozen of our closest friends and family, gathered for a night of fun. The kids were enjoying the party favors:\n",
            "The room was decorated with the help of our friend Michelle (who also created the lovely invitation):\n",
            "The food was amazing:\n",
            "And the dessert table was a wonderful place for a late night snack:\n",
            "And of course, the best part of any party is the gifts!\n",
            "We received some amazing gifts and cards, and we are so grateful for everyone’s generosity. The kids will have lots of fun with their new toys and we’re looking forward to using their gifts too!\n",
            "We’ve been thinking a lot about the party and our baby girl, who is now 1 month old. This is such an exciting time, and we are so grateful to have such great friends and family to celebrate our lives with!\n",
            "And we’re so grateful to everyone who made the party happen. It was a wonderful time, and we can’t wait to see everyone again!\n",
            "(And thanks for all the wonderful gifts, too!)\n",
            "Pingback: Top 10 Dining Tables for Small Spaces\n",
            "Pingback: Top 10 Dining Tables for Small Spaces\n",
            "Pingback: Top 10 Dining Tables for Small Spaces\n",
            "Pingback: Top 10 Dining Tables for Small Spaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = tokenizer(\"The price of our widgets is too high, said the customer...\", return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "new_tokens = model.generate(\n",
        "    **new_prompt,\n",
        "    max_new_tokens=1000,\n",
        "    temperature=0.75,\n",
        "    top_p=0.95,\n",
        "    do_sample=True,\n",
        ")\n",
        "print(tokenizer.decode(new_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h20F9KIm7YFF",
        "outputId": "302970bf-e45c-483b-d586-eee7ef51f9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The price of our widgets is too high, said the customer...\n",
            "...and we agreed.\n",
            "The customer's widget was not going to be the one to make it big, so we were happy to cut our losses.\n",
            "Here's the first of the three versions of the widget. The price is too high, we can't make money off it, but we think it's a good design.\n",
            "Here's version two. We took the price down to $1.95. Not great, but we're happy to take $1.95.\n",
            "And here's version three. We reduced the price to $1.\n",
            "And we agreed.\n",
            "We were right.\n",
            "The widget was a flop.\n",
            "The customer wasn't a bad guy. He just didn't have the vision to see that his widget would never make it.\n",
            "But we do.\n",
            "And we're willing to cut our losses and move on to the next widget.\n",
            "The only thing we don't see is the next widget.\n",
            "Maybe we should ask our customer for advice.\n",
            "But that would be too easy.\n",
            "We're better off just to stay in our comfort zone.\n",
            "We'll never make it big.\n"
          ]
        }
      ]
    }
  ]
}